\documentclass[twocolumn,landscape,10pt]{article}
\usepackage[thinc]{esdiff} % for typesettign derivatives
\usepackage{amsthm} % provides an enhanced version of LaTex's \newtheorem command
\usepackage{mdframed} % framed environments that can split at page boundaries
\usepackage{enumitem} % bulletin points or other means of listing things
\usepackage{dsfont} % for symbols
\usepackage{amssymb} % for AMS symbols
\usepackage{amsmath} % so as to use align
\usepackage{latexsym} % so as to use symbols like \leadsto
\usepackage{mathrsfs} % for using mathscr for char like operators
\usepackage{commath} % for using norm symbol
\usepackage{mathtools} % for using environments like dcases
\usepackage{authblk} % for writing affiliations
\usepackage{graphicx} % for importing images
\graphicspath{{./images/}} % for the path to images, also always put label behind captions
\usepackage{textcomp} % for using degree symbol
\usepackage{hyperref} % for clickable link in the pdf & customizable reference text
\usepackage[all]{hypcap} % for clickable link to images instead of caption
\usepackage[margin=0.9in]{geometry} % default is 1.5in
% \usepackage[left=0.4in, right=0.4in, top=0.8in, bottom=0.8in]{geometry}
\usepackage[title]{appendix} % for attaching appendix
\allowdisplaybreaks % allow page breaking in display maths, like align
\usepackage{xcolor} % for setting color of a block of text, use \textcolor{<color>}{}
\usepackage[normalem]{ulem} % for strikethrough text, use \sout{}
% allow for more advanced table layout
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
% for adjusting caption settings
\usepackage{caption}
\captionsetup[table]{skip=10pt}

\theoremstyle{definition}
\mdfdefinestyle{defEnv}{%
  hidealllines=false,
  nobreak=true,
  innertopmargin=-1ex,
}

% The following is for writing block of code
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% setting of the thickness of the 4 lines of box
\setlength{\fboxrule}{2pt}

% Use the following to change code language and related settings
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  literate={~} {$\sim$}{1}
}

\pagestyle{headings}
\author{Lectured by Paul Bilokun}
\title{Reinforcement Learning}
\affil{Typed by Aris Zhu Yi Qing}
\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Markov Concept Definitions}

\begin{itemize}
    \item \textbf{\underline{Markov Process}}: a tuple
        $(\mathcal{S},\mathcal{P})$, where:
        \begin{itemize}
            \item $\mathcal{S}$ -- a set of states
            \item $\mathcal{P}_{ss'} = P[S_{t+1}=s'|S_t=s]$ -- a state
                transition probability matrix
            \item $\sum_{s'}\mathcal{P}_{ss'}=1$ (to satisfy the probability axiom)
        \end{itemize}
    \item A state $s_t$ is \textbf{\underline{Markov}} $\iff$
        $P[s_{t+1}|s_t]=P[s_{t+1}|s_1,\ldots,s_t]$
        \begin{itemize}
            \item once the state is known, then any data of the history is
                no longer needed
        \end{itemize}
    \item \textbf{\underline{Stationarity (Homogeneous)}}: $P[s_{t+1}|s_t]$
        doesn't depend on $t$, but only on the origin and destination states.
    \item \textbf{\underline{Markov Reward Process (MRP)}}: a tuple
        $(\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma)$, where:
        \begin{itemize}
            \item $\mathcal{S}$ -- a set of states
            \item $\mathcal{P}_{ss'}$ -- a state transition probability matrix
            \item $\mathcal{R}_s = \mathbb{E}[r_{t+1}|S_t=s]$ -- an expected
                immediate reward that we collect upon departing state $s$, whose
                collection occurs at time step $t+1$
            \item $\gamma\in[0,1]$ -- a discount factor
        \end{itemize}
    \item \textbf{\underline{Return $(R_t)$}}: the total \emph{discounted} 
        reward from time-step $t$:
        \[
            R_t=r_{t+1}+\gamma r_{t+1}+\cdots=\sum_{k=0}^{\infty} \gamma^k r_{t+k+1}
        \]
        \begin{itemize}
            \item $\gamma$ close to 0 leads to \underline{myopic} evaluation
            \item $\gamma$ close to 1 leads to \underline{far-sighted}
                evaluation
            \item if $T$ is the time to reach the terminal state, then
                \[
                    R_1=r_2+\gamma r_3+\cdots+\gamma^{T-2}r_T.
                \]
        \end{itemize}
    \item \textbf{\underline{State Value Function $v(s)$}} of an MRP: the
        expected return $R$ starting from state $s$ at time $t$:
        \[
            v(s)=\mathbb{E}[R_t|S_t=s].
        \]
    \item \textbf{\underline{Bellman Equation}} for MRPs:
        \begin{align*}
            v(s)
            &= \mathbb{E}[R_t|S_t=s] \\
            &= \mathbb{E}[r_{t+1}+\gamma(r_{t+2}+\gamma r_{t+3}+\cdots)|S_t=s] \\
            &= \mathbb{E}[r_{t+1}+\gamma R_{t+1}|S_t=s] \\
            &= \mathbb{E}[r_{t+1}+\gamma v(S_{t+1})|S_t=s]
        \end{align*}
        i.e.\ $v(s)$ decomposes into
        \begin{itemize}
            \item immediate reward $r_{t+1}$
            \item discounted return of successor state $\gamma v(S_{t+1})$.
        \end{itemize}
        Alternative forms:
        \begin{itemize}
            \item sum notation:
                $v(s)=\mathcal{R}_s+\gamma\sum_{s'\in\mathcal{S}}\mathcal{P}_{ss'}v(s')$
            \item \textbf{\underline{vector notation}}:
                $\mathbf{v}=\mathcal{R}+\gamma\mathcal{P}\mathbf{v}$, where
                $\mathbf{v}\in\mathbb{R}^{n}$.
        \end{itemize}
        Solving the equation in vector notation, we obtain
        \[
            \mathbf{v}=(\mathds{1}-\gamma\mathcal{P})^{-1}\mathcal{R}
        \]
        while the iterative methods to solve this include:
        \begin{enumerate}
            \item dynamic programming
            \item Monte-Carlo evaluation
            \item temporal-difference learning
        \end{enumerate}
    \item \textbf{\underline{Policy}}: the
        conditional probability distribution to execute an action
        $a\in\mathcal{A}$ given that one is in state $s\in\mathcal{S}$ at time
        $t$:
        \[
            \pi_t(a,s)=P[A_t=a|S_t=s]
        \]
        This is considered as \textbf{\underline{probabilistic}} or
        \textbf{\underline{stochastic}}.
        \begin{itemize}
            \item Policy is \textbf{\underline{deterministic}} if
                $\pi(a,s)=1$ and $\pi(a',s)=0$ $\forall a\neq a'$.
            \item Alternative notation for deterministic policy: $\pi_t(s)=a$.
        \end{itemize}
        Optimal policy (action) maximises expected return.
\end{itemize}

\section{Markov Decision Process}

\subsection{Definition}

\begin{itemize}
    \item $\mathcal{S}$ -- State space
    \item $\mathcal{A}$ -- Action space
    \item $\mathcal{P}_{ss'}^{a}$ -- transition probability $p(s_{t+1}|s_t,a_t)$
    \item $\gamma\in[0,1]$ -- discount factor
    \item $\mathcal{R}_{ss'}^{a}=r(s,a,s')$ -- immediate/instantaneous reward
        function.
        \begin{itemize}
            \item temporal notation: $r_{t+1}=r(s_{t+1},s_t,a_t)$
        \end{itemize}
    \item $\pi$ -- policy
        \begin{itemize}
            \item stochastic: $\mathbf{a}\sim p_\pi(\mathbf{a}|\mathbf{s})
                \equiv \pi(\mathbf{a}|\mathbf{s}) \equiv \pi(a,s)$
                \begin{itemize}
                    \item each entry of the distribution is $\pi(a_1|\mathbf{s})$
                        or $\pi(a_1|s)$, depending on whether it is a collection
                        of states of different objects $\mathbf{s}$ or just
                        state of one object $s$.
                \end{itemize}
            \item deterministic: $\mathbf{a}=\pi(\mathbf{s})$
        \end{itemize}
\end{itemize}

\subsection{State Value Function (Bellman Equation)}

\begin{align*}
    V^{\pi}(s)
    &= \mathbb{E}[R_t|S_t=s] \\
    &= E\left[\sum_{k=0}^{\infty}\gamma^{k}r_{t+k+1}|S_t=s\right] \\
    &= E\left[r_{t+1}+\gamma\sum_{k=0}^{\infty}\gamma^{k}r_{t+k+2}|S_t=s\right] \\
    &=
    \sum_{a\in\mathcal{A}}\pi(a,s)\left\{\sum_{s'\in\mathcal{S}}\mathcal{P}_{ss'}^a\left(
    \mathcal{R}_{ss'}^a+\gamma\mathbb{E}_\pi\left[
\sum_{k=0}^{\infty}\gamma^{k}r_{t+k+2}|S_{t+1}=s'\right]\right)\right\} \\
    &=
    \sum_{a\in\mathcal{A}}\pi(a,s)\left\{\sum_{s'\in\mathcal{S}}\mathcal{P}_{ss'}^a\left(
    \mathcal{R}_{ss'}^a+\gamma V^\pi(s')\right)\right\}
\end{align*}
where, for instance,
\[
    \mathbb{E}[r_{t+1}|S_t=s]=\sum_{a\in\mathcal{A}}P[a|s]\left(\sum_{s'\in\mathcal{S}}P[s'|s,a]r(s,a,s')\right).
\]
On the other hand, the \textbf{\underline{Bellman Optimality Equation}} for $V$
is
\[
    V^*(s)=\underset{a}{\max}\sum_{s'}\mathcal{P}_{ss'}^a(\mathcal{R}_{ss'}^a+\gamma
    V^*(s'))
\]
saying that the value of a state under an optimal policy $\pi^*$ must equal the
expected return for the best action from that state.

\subsection{Policy Evaluation (Prediction Problem)}

\begin{itemize}
    \item \textbf{\underline{Iterative}} policy evaluation:
        $V_1(s),V_2(s),\ldots,V_k(s)$
        \begin{itemize}
            \item Pseudocode: \label{code:1}
                
                Input $\pi$ the policy to be evaluated\\
                Initialize $V(s)=0$, $\forall s\in\mathcal{S}^+$\\
                Repeat\\
                \hspace*{2ex} $\Delta \leftarrow 0$\\
                \hspace*{2ex} For each $s\in\mathcal{S}$: \\
                \hspace*{2ex}\hspace*{2ex} $v\leftarrow V(s)$\\
                \hspace*{2ex}\hspace*{2ex} $V(s)\leftarrow
                \sum_a\pi(s,a)\sum_{s'}\mathcal{P}_{ss'}^a[\mathcal{R}_{ss'}^a+\gamma
                V(s')]$\\
                \hspace*{2ex}\hspace*{2ex} $\Delta\leftarrow
                \max{(\Delta,|v-V(s)|)}$\\
                until $\Delta<\theta$ (a small positive number as a threshold)\\
                Output $V\approx V^\pi$
        \end{itemize}
\end{itemize}

\subsection{State-Action Value Function (Cost-To-Go)}

\begin{itemize}
    \item Definition:
        \[
            Q^\pi(s,a)=\mathbb{E}[R_t|S_t=s,A_t=a]
            =\mathbb{E}\left[\sum_{k=0}^{\infty}\gamma^k r_{t+k+1}|S_t=s,A_t=a\right]
        \]
    \item relation to State Value Function:
        \[
            V^\pi(s)=\sum_{a\in\mathcal{A}}\pi(s,a)Q^\pi(s,a)
        \]
    \item \textbf{\underline{Bellman Optimality Equation}} for $Q^\pi$:
        \[
            Q^*(s,a)=\sum_{s'}\mathcal{P}_{ss'}^a\left[\mathcal{R}_{ss'}^a+\gamma\,
            \underset{a'}{\max}\;Q^*(s',a')\right]
        \]
\end{itemize}

\subsection{Optimal Value Function}

\begin{itemize}
    \item \textbf{\underline{Optimal State Value Function}}:
        \[
            V^*(s)=\underset{\pi}{\max}\;V^\pi(s),\;\forall s\in\mathcal{S}
        \]
        The policy $\pi^*$ that maximises the value function is the
        \textbf{\underline{optimal policy}}.
    \item \textbf{\underline{Optimal State-Action Value Function}}:
        \[
            Q^*(s,a)=\underset{\pi}{\max}\,Q^\pi(s,a),\;\forall
            s\in\mathcal{S},a\in\mathcal{A}
        \]
        which then we also have
        \[
            Q^*(s,a)=\mathbb{E}[r_{t+1}+\gamma V^*(s_{t+1})|S_t=s,A_t=a].
        \]
\end{itemize}

\subsection{Bellman Optimality Equation Convergence Theorem}

For an MDP with a finite state and action space
\begin{enumerate}
    \item The Bellman (Optimality) equations have a unique solution
    \item The values produced by value iteration converge to the solution of the
        Bellman Equation
\end{enumerate}

\section{Dynamic Programming}

\subsection{Policy Improvement}

\begin{itemize}
    \item Let $\pi$ and $\pi'$ be any two deterministic policies s.t.
        $\forall s\in\mathcal{S},$ $Q^\pi(s,\pi'(s))\ge V^\pi(s)$,
    \item then $\pi'$ must be as good or better than $\pi$, i.e.\ 
        $\forall s\in\mathcal{S}$, $V^{\pi'}(s)\ge V^\pi(s)$.
\end{itemize}

\subsection{Policy Iteration}

\begin{itemize}
    \item Once a policy $\pi$ has been improved to be 
        $\pi'$ s.t. $V^\pi \le V^{\pi'}$, and continues,
        this forms a sequence of monotonically improving policies and value functions.
    \item This eventually finds the optimal policy $\pi^*$,
        and the process is called \underline{policy iteration}.
    \item Pseudocode:
        
        \underline{Policy evaluation}: same as above \ref{code:1}.

        \underline{Policy Improvement}:\\
        policy-stable $\leftarrow$ true\\
        For each $s\in\mathcal{S}$:\\
        \hspace*{2ex}$b\leftarrow\pi(s)$\\
        \hspace*{2ex}$\pi(s)\leftarrow
        \underset{a}{\text{argmax}}\sum_{s'}\mathcal{P}_{ss'}^a\left[\mathcal{R}_{ss'}^a+\gamma
        V(s')\right]$\\
        \hspace*{2ex}If $b\neq\pi(s)$, then policy-stable $\leftarrow$ false\\
        If policy-stable, then stop; else go to evaluation.

    \item 
        What the \underline{Policy Improvement} step is doing is that it takes the optimal
        action $a'$ from the current state to the next state $s'$, which forms
        the new policy $\pi'$, which in the next iteration find the optimal
        action $a''$ forming the new policy $\pi''$, etc.
\end{itemize}

\subsection{Bellman's Priciple of Optimality}

A policy $\pi(a|s)$ achieves the optimal value from state $s$,
$V^\pi(s')=V^*(s')$, iff
\begin{itemize}
    \item for any state $s'$ reachable from $s$,
    \item $\pi$ achieves the optimal value from state $s'$, $V^\pi(s')=V^*(s')$.
\end{itemize} 

\subsection{Value Iteration}

\begin{itemize}
    \item Pseudocode:

        Initialize $V$ arbitrarily $\forall s\in\mathcal{S}^+$.

        Repeat\\
        \hspace*{2ex}$\Delta\leftarrow 0$\\
        \hspace*{2ex}For each $s\in\mathcal{S}$:\\
        \hspace*{4ex}$v\leftarrow V(s)$\\
        \hspace*{4ex}$V(s)\leftarrow\pmb{\max}_a\sum_{s'}\mathcal{P}_{ss'}^a\left[\mathcal{R}_{ss'}^a+\gamma
        V(s')\right]$\\
        \hspace*{4ex}$\Delta\leftarrow\max{(\Delta,|v-V(s)|)}$\\
        until $\Delta<\theta$ (a small positive number)

        Then perform \underline{Policy Extraction} s.t.
        \[
            \pi(s)=\underset{a}{\text{argmax}}\sum_{s'}\mathcal{P}_{ss'}^a\left[\mathcal{R}_{ss'}^a+\gamma
            V(s')\right].
        \]
    \item Policy iteration v.s. Value iteration:
        \begin{itemize}
            \item \textbf{Policy iteration} includes: policy evaluation + policy
                improvement
            \item \textbf{Value iteration} includes: finding optimal value
                function + one policy extraction.
            \item Finding optimal value function can be seen as a combination of
                policy improvement and policy evaluation
            \item Policy iteration and finding optimal value function are highly
                similar except for a max operation.
            \item It is more likely that policy iteration is faster than value
                iteration due to faster convergence.
        \end{itemize}
    \item This is similar to what we normally understand about dp coding
        problems, where at each iteration we look for optimal computed value
        (although most likely we will figure out the optimal policy first).
\end{itemize}

\subsection{Sync/Async backups}

\begin{itemize}
    \item \textbf{\underline{Synchronous backups}}: 
        \begin{itemize}
            \item all states are backed up in parallel,
            \item thereby requiring two copies of the value function
        \end{itemize}
    \item \textbf{\underline{Asynchronous backups}}:
        \begin{itemize}
            \item not going through all states,
            \item or not in any ordering,
            \item with any available state/state-action values
            \item one copy of value function, in-place update
            \item e.g. value iteration with only one state updated per iteration
        \end{itemize}
\end{itemize}

\section{Model-Free Learning}

\subsection{Monte Carlo (MC) Learning}

\subsubsection{Introduction}

\begin{itemize}
    \item MC methods learn directly form episodes of experience
    \item MC is \textbf{\underline{model-free}}: no knowledge of MDP 
        transitions/rewards needed
    \item MC learns from complete episodes: no bootstrapping, i.e.\ not updating
        value estimates based on other value estimates
    \item main idea: value of staste = mean return
        \begin{itemize}
            \item an obvious way thus is to estimate from experience -- average
                the returns observed after visits to that state
            \item as more returns are observed, the average should converge to
                the expected value
        \end{itemize}
    \item can only be applied to \textbf{\underline{episodic}} MDPs that have
        terminal states
\end{itemize}

\subsubsection{MC Policy Evaluation}

\begin{itemize}
    \item Goal: learn $V^\pi$ from traces $\tau$ of episodes of length $T$ that
        we experience under policy $\pi$, where
        \[
            \tau\equiv s_1,a_1,r_1, s_2,\ldots,s_k.
        \]
    \item Pseudocode:

        $\hat{V}(s)\leftarrow$ arbitrary value, $\forall s\in\mathcal{S}$\\
        Returns($s$) $\leftarrow$ an empty list, $\forall s\in\mathcal{S}$

        \textbf{repeat}\\
        \hspace*{2ex}Get trace $\tau$ using $\pi$\\
        \hspace*{2ex}\textbf{for all} $s$ appearing in $\tau$ \textbf{do}\\
        \hspace*{4ex}$R\leftarrow$ return from first appearance of $s$ in
        $\tau$.\\
        \hspace*{4ex}Append $R$ to Returns($s$)\\
        \hspace*{4ex}$\hat{V}(s)\leftarrow$ average(Returns($s$))\\
        \textbf{until} forever
\end{itemize}


\end{document}
