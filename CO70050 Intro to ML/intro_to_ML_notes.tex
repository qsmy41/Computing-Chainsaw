\documentclass[twocolumn,landscape,10pt]{article}
\usepackage[thinc]{esdiff} % for typesettign derivatives
\usepackage{amsthm} % provides an enhanced version of LaTex's \newtheorem command
\usepackage{mdframed} % framed environments that can split at page boundaries
\usepackage{enumitem} % bulletin points or other means of listing things
\usepackage{amssymb} % for AMS symbols
\usepackage{amsmath} % so as to use align
\usepackage{latexsym} % so as to use symbols like \leadsto
\usepackage{mathrsfs} % for using mathscr for char like operators
\usepackage{commath} % for using norm symbol
\usepackage{mathtools} % for using environments like dcases
\usepackage{authblk} % for writing affiliations
\usepackage{graphicx} % for importing images
\graphicspath{{./images/}} % for the path to images, also always put label behind captions
\usepackage{textcomp} % for using degree symbol
\usepackage{hyperref} % for clickable link in the pdf & customizable reference text
\usepackage[all]{hypcap} % for clickable link to images instead of caption
\usepackage[margin=1in]{geometry} % default is 1.5in
% \usepackage[left=0.4in, right=0.4in, top=0.8in, bottom=0.8in]{geometry}
\usepackage[title]{appendix} % for attaching appendix
\allowdisplaybreaks % allow page breaking in display maths, like align
% allow for more advanced table layout
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
% for adjusting caption settings
\usepackage{caption}
\captionsetup[table]{skip=10pt}

\theoremstyle{definition}
\mdfdefinestyle{defEnv}{%
  hidealllines=false,
  nobreak=true,
  innertopmargin=-1ex,
}

% The following is for writing block of code
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% setting of the thickness of the 4 lines of box
\setlength{\fboxrule}{2pt}

% Use the following to change code language and related settings
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\pagestyle{headings}
\author{Lectured by Josiah Wang}
\title{CO70050 Introduction to Machine Learning}
\affil{Typed by Aris Zhu Yi Qing}
\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Definitions}

\begin{enumerate}
    \item \textbf{Artificial Intelligence}:
        Techniques that enable computers to mimic human behaviour and
        intelligence. It could be using logic, if-then rules, machine learning,
        etc.
    \item \textbf{Machine Learning(ML)}:
        Subset of AI techniques using statistical methods that enable the
        systems to learn and improve with experience.
        \begin{itemize}
            \item More data means more accurate predictions.
            \item Select/Extract good features for predictions. 
                more feature $\nRightarrow$ better prediction (curse of
                dimensionality: increased computational complexity, data sparsity,
                overfitting)
            \item Pipeline: feature encoding, ML algorithm, and evaluation.
        \end{itemize} 
    \item \textbf{Deep Learning}:
        Subset of machine learning techniques using multi-layer Artificial
        Neural Networks(ANN) and vast amounts of data for learning.
    \item \textbf{Supervised learning}:
        Take input variables and correct output labels as \underline{inputs}, 
        feed them into a supervised \underline{learning algorithm} 
        to generate a \underline{model} which can be used to
        \underline{estimate labels} of other input variables.
        \begin{itemize}
            \item \textbf{Semi-supervised learning}:
                Some data have labels, some do not.
            \item \textbf{Weakly-supervised learning}:
                Inexact output labels.
        \end{itemize} 
    \item \textbf{Unsupervised learning}:
        Take input variables only, feed them into an unsupervised learning
        algorithm to generate a model which can be used to estimate labels of
        other input variables.
        \begin{itemize}
            \item discover hidden/latent structure within the data (``lossy data
                compression'')
        \end{itemize} 
    \item \textbf{Reinforcement learning}:
        Largely the same as unsupervised learning, except that
        the estimated labels at the end ``interact with an
        environment'' and send reward signal back to the reinforcement learning
        algorithm such that the algorithm will take the reward signal into
        consideration when learning the model next time.
        \begin{itemize}
            \item find which action an agent should take, depending on 
                its current state, to maximise the received rewards (Policy
                search)
        \end{itemize} 
    \item \textbf{Classification}:
        The task of approximating a mapping function from input variables to
        \underline{discrete} output variables.
        \begin{itemize}
            \item Binary classification: only two possible cases.
            \item Multi-class classification: more than one possible class to
                choose from, but every input belongs to exactly one class.
            \item Multi-label classification: Each input can belong to more than
                one class.
        \end{itemize} 
    \item \textbf{Regression}:
        The task of approximating a mapping function from input variables to
        \underline{continuous} output variables.
    \item \textbf{Lazy Learner}:
        Stores the training examples and postpones generalising beyond these
        data until an explicit request is made at test time.
    \item \textbf{Eager Learner}:
        Constructs a general, explicit description of the target function based
        on the provided training examples.
    \item \textbf{Non-parametric model}: Assume that data distribution cannot be
        defined in terms of a finite set of parameters.
        The distribution depends on the data themselves.
    \item \textbf{Underfitting/high bias}: a lot of errors, oversimplified
        assumptions.
    \item \textbf{Overfitting/high variance}: fits ``perfectly'' the training
        data, and may not fit the test data well.
\end{enumerate} 

\section{Classification}

\subsection{Instance-based Learning}

\subsubsection{$k$ Nearest Neighbours ($k$-NN) classifier}

\begin{enumerate}
    \item Non-parametric model
    \item Lazy learner
    \item Procedure: Obtain $k$ nearest data, classify the instance under the
        class which the most number of neighbours belong to.
    \item $k$ is usually an odd number.
    \item Increasing $k$ will make the classifier
        \begin{itemize}
            \item have a smoother decision boundary (higher bias)
            \item less sensitive to training data (lower variance)
        \end{itemize} 
    \item Various distance metrics, such as:
        \begin{itemize}
            \item Manhattan distance ($L^{1}$-norm):
                \[
                    d(x^{(i)},x^{(q)})=\sum_{k=1}^{K}
                    \left|x_k^{(i)}-x_k^{(q)}\right|,
                \]
            \item Euclidean distance ($L^{2}$-norm):
                \[
                    d(x^{(i)},x^{(q)})=\sqrt{\sum_{k=1}^{K}
                    {\left(x_k^{(i)}-x_k^{(q)}\right)}^{2}},
                \]
            \item Chebyshev distance ($L^{\infty}$-norm):
                \[
                    d(x^{(i)},x^{(q)})=\text{max}_{k=1}^{K}\left|x_k^{(i)}-x_k^{(q)}\right|.
                \]
        \end{itemize} 
    \item the curse of dimensionality
        \begin{itemize}
            \item $k$-NN relies on distance metrics, which may not work well if
                using all features in high dimensional space.
            \item If many features are irrelevant, instances belonging to the
                same class may be far from each other.
            \item Solution: Weight each feature differently, or perform feature
                selection/extraction.
        \end{itemize} 
\end{enumerate} 

\subsubsection{Distance-weighted $k$-NN}

\begin{enumerate}
    \item Any measure favouring the votes of nearby neighbours will work, such
        as:
        \begin{itemize}
            \item Inverse of distance: $w^{(i)}=\frac{1}{d(x^{(i)},x^{(q)})}$,
            \item Gaussian distribution:
                $w^{(i)}=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{d(x^{(i)},x^{(q)})^{2}}{2}\right)$.
        \end{itemize} 
    \item $k$ is not so important in distance-weighted $k$-NN. Distant data will
        have small weights and won't greatly affect classification.
    \item if $k=N$ (size of the training set): global method. Otherwise, it is a
        local method.
    \item Robust to noisy training data --- the impact of isolated noise is
        smoothened out.
    \item simple yet powerful, but might be slow for large datasets
\end{enumerate} 

\subsubsection{$k$-NN regression (quick intro)}

\begin{enumerate}
    \item compute the \underline{(weighted) mean value} across $k$ nearest neighbours.
\end{enumerate} 

\subsection{Decision Trees}

\subsubsection{Intuitions and Introduction}

\begin{enumerate}
    \item Eager learner
    \item Decision Tree learning (or construction/induction) is a method for
        approximating discrete classification functions by means of a tree-based
        representation.
    \item A decision tree can be represented as a set of if-then rules.
    \item Decision Tree learning algo employ top-down greedy search through the
        space of possible solutions.
    \item General Algorithm:
        \begin{enumerate}
            \item \label{dt1}
                Search for an `optimal' splitting rule on training data.
            \item \label{dt2}
                Split your dataset according to the chosen splitting rule
            \item Repeat \ref{dt1} and \ref{dt2} on each new splitted subset
                unless the subset contains data of only one class.
        \end{enumerate} 
\end{enumerate} 

\subsubsection{Selecting the optimial splitting rule}

\begin{enumerate}
    \item several metrics:
        \begin{itemize}
            \item Information gain: Used in ID3, C4.5
                \begin{itemize}
                    \item quantifies the reduction of information entropy
                \end{itemize} 
            \item Gini impurity: Used in CART
            \item Variance reduction: Used in CART
        \end{itemize} 
    \item information entropy:
        \begin{itemize}
            \item \textbf{Information Content} $I$ is a quantity derived from the
                probability of an event occurring from a random variable $X$ as
                \[
                    I(x)=\log_2(\frac{1}{P(x)})=-\log_2(P(x)),
                \]
                which could be interpreted as the amount of information required
                to fully determine the state of a random variable, and the
                definition should satisfy several conditions as specified
                \href{https://en.wikipedia.org/wiki/Entropy_(information_theory)#Characterization}{here}.
            \item \textbf{Information Entropy} $H$ of a discrete random variable
                $X$ with its p.m.f. being $P(X)$ is defined as the 
                expected amount of information content as following:
                \[
                    H(X)=E[I(X)]=-\sum_{k=1}^{n} P(x_k)\log_2(P(x_k)),
                \]
                where $n$ is the number of possible discrete values of $X$.
                For a p.d.f. $f(X)$, we can define the
                \textbf{continuous entropy} as
                \[
                    H(X)=-\int_{x} f(x)\log_2(f(x))\mathrm{d}x.
                \]
                \begin{itemize}
                    \item The analogy in continuous case is imperfect (it can
                        have negative values) but is still often used in deep
                        learning.
                    \item The p.d.f. is often unknown, but can be approximated
                        with density estimation algorithms for instance.
                \end{itemize} 
        \end{itemize} 
    \item Use Information Entropy to select the `optimal' split rule.
        \begin{itemize}
            \item \textbf{Information Gain (IG)} is the difference between the
                initial entropy and the (weighted) average entropy of the
                produced subsets.
            \item Mathematically,
                \[
                    \text{IG}(D, S) = H(D) - \sum_{s\in S} \frac{|s|}{|D|}H(s),
                \]
                where $D$ stands for the dataset, $S$ stands for the subsets
                after splitting, and $|\cdot|$ is the cardinality operator.
        \end{itemize} 
\end{enumerate} 

\subsubsection{Comments}

\begin{enumerate}
    \item prevent overfitting
        \begin{itemize}
            \item Early stopping, e.g. max depth, min examples.
            \item Pruning
                \begin{enumerate}[label = (\roman*)]
                    \item Go through nodes which are connected only to leaf
                        nodes.
                    \item Turn each into a leaf node (with majority class
                        label).
                    \item Evaluate pruned tree on validation set. Prune if
                        accuracy higher than unpruned.
                    \item Repeat until all such nodes have been tested.
                \end{enumerate} 
        \end{itemize} 
    \item Random Forests
        \begin{itemize}
            \item Many decision trees voting on the class label.
            \item Each tree generated with random samples of training set
                (bagging) and random subset of features.
        \end{itemize} 
    \item Regression Trees
        \begin{itemize}
            \item Instead of a class label, each leaf node now predicts an
                $x\in\mathbb{R}$.
            \item Use a different metric for splitting, e.g. variance reduction.
        \end{itemize} 
\end{enumerate} 

\section{ML Evaluation}

\subsection{Common Pipeline}

\begin{enumerate}
    \item shuffle the initial dataset to get rid of potential implicit ordering
    \item split shuffled dataset to larger training dataset and smaller test
        dataset
    \item use training dataset to train the model
    \item feed test dataset into the trained model to evaluate the model's performance
\end{enumerate} 

\subsection{Hyperparameter Tuning}

\begin{itemize}
    \item \underline{Hyperparameter} is defined as the model parameter that are
        chosen before the training, such as the $k$ of the $k$-NN algorithm.
    \item Overall objective is to find the hyperparameter values that lead to
        the best performance
    \item The correct approach is to split the dataset into 3:
        training/validation/test, with common splits between 6:2:2 and 8:1:1.
    \item It's possible to merge validation set into training set to train the
        model again with more data after identifying the best hyperparameter
        using the validation set. This can improve performance.
\end{itemize} 

\subsection{Corss Validation}

\begin{enumerate}
    \item used when dataset is small
    \item divide dataset into $k$ (usually 10) equal folds/splits: use $k-1$
        folds for training and 1 for testing
    \item iterate $k$ times, each time test on different portion of data
    \item performance on all $k$ held-out test sets can be average:
        \[
            \text{Global error estimate} = \frac{1}{N}\sum_{i=1}^{N} e_i
        \]
    \item \underline{Caution}: The estimate is for \emph{algorithm} accuracy,
        not \emph{model} accuracy.
    \item While tuning hyperparameter, doing the same thing as with large
        datasets at each iteration.
\end{enumerate} 

\subsection{Evaluation Metrics}

\begin{itemize}
    \item Confusion Matrix:
        \[
            \begin{pmatrix}
                \text{True Positive} & \text{False Negative}\\
                \text{False Positive} & \text{True Negative}
            \end{pmatrix} \quad\text{OR}\quad
            \begin{pmatrix}
                \text{TP} & \text{FN}\\
                \text{FP} & \text{TN}
            \end{pmatrix}
        \]
        \[
            \text{OR }\begin{pmatrix}
                \text{TP} & \text{FN} & \text{FN} & \text{FN}\\
                \text{FP} & \text{TN} & \text{?} & ?\\
                \text{FP} & \text{?} & \text{TN} & ?\\
                \text{FP} & \text{?} & \text{?} & \text{TN}\\
            \end{pmatrix} \text{ for Class 1 in multiple classes}
        \]
    \item Metrics:
        \begin{align*}
            \text{Accuracy}&=\frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FN}+\text{FP}}\\
            \text{Classification error} &= 1 - \text{accuracy}\\
            \text{Precision}&=\frac{\text{TP}}{\text{TP}+\text{FP}}\\
            \text{Recall}&=\frac{\text{TP}}{\text{TP}+\text{FN}}\\
            \text{Macro-averaged }\left\langle\text{metric}\right\rangle&
            =\frac{1}{N}\sum_{i=1}^{N} \left\langle\text{metric}\right\rangle_i\\
            \text{Micro-averaged precision}&=\frac{\sum_{i=1}^{N} \text{TP}_i}
            {\sum_{i_1}^{N} \text{TP}_i + \text{FP}_i},
            \quad\text{same for other metrics}\\
            F_1&=\frac{2*\text{precision}*\text{recall}}{\text{precision}+\text{recall}}\\
            F_\beta&=(1+\beta^2)*\frac{\text{precision}*\text{recall}}{(\beta^2*\text{precision})+\text{recall}}\\
            \text{MSE}&=\frac{1}{N}\sum_{i=1}^{N}
            {\left(Y_i-\hat{Y}_i\right)}^{2}\\
            \text{Root MSE}&=\sqrt{\text{MSE}}
        \end{align*} 
    \item Precision v.s. Recall:
        \begin{itemize}
            \item high recall, low precision:
                Most of the positive examples are identified correctly (low FN)
                but there are many FPs.
            \item high precision, low recall:
                We miss a lot of positive examples (high FN) but those we
                predict as positive are really positive (low FP).
        \end{itemize} 
\end{itemize} 

\subsection{Imbalanced Data Distribution}

\paragraph{Problem}
\begin{itemize}
    \item Imbalanced dataset: classes are not equally represented
    \item accuracy is misleading, affected a lot by the majority class
    \item Macro-averaged recall can help detect if a class is misclassified, but
        no info about FP. Similarly for precision.
    \item we should look at several matrices, along with the confusion matrix
\end{itemize} 

\paragraph{Solution}
\begin{itemize}
    \item normalized matrix: divide each row by the total number of samples in
        that class
    \item downsample the majority class/upsample the minority class
    \item Look at differet metrics and choose ones that reflect the intended
        behaviour of the model.
\end{itemize} 

\subsection{Overfitting}

\begin{itemize}
    \item \underline{Overfitting}: good performance on training data, poor
        generalization to other data, can occur when:
        \begin{itemize}
            \item the model we use is too complex (use right level of
                complexity)
            \item the examples in the training set are not representative of all
                possible situations (get more data)
            \item learning is performed for too long (stop training earlier)
        \end{itemize} 
    \item \underline{underfitting}: poor performance on the training data and
        poor generalization to other data
\end{itemize} 

\subsection{Confidence Intervals}

\begin{itemize}
    \item \underline{True error} of model $h$ is the probability that it
        misclassifies a randomly drawn example $x$ from distribution $D$:
        \[
            \epsilon_D(h):=P[f(x)\neq h(x)].
        \]
    \item \underline{Sample error (classification error)} of the model h based on a data sample $S$:
        \[
            \epsilon_S(h):=\frac{1}{N}\sum_{x\in S} \delta(f(x),h(x))
        \]
    \item \underline{Confidence interval} is defined as,
        when given a sample $S$ with the number of test samples $n\ge 30$,
        we can say that with $N\%$ confidence,
        the true error lies in the interval:
        \[
            \epsilon_S(h)\pm Z_N\sqrt{\frac{\epsilon_S(h)*(1-\epsilon_S(h))}{n}}
        \]
        where
        \[
            \Phi(S'\le\left|Z_N\right|)=N\%
        \]
        and $S'$ is the normalized sample of $S$.
\end{itemize} 

\subsection{Testing for Statistical Significance}

\begin{itemize}
    \item The \underline{statistical tests} tell us if there is indeed a
        difference between the two distributions, such as Randomisation test,
        $T$-test, Wilcoxon rank-sum test, etc.
    \item The \underline{null hypothesis $H_0$} states that the two algorithms/models
        perform the same and the performance differences are only due to
        sampling error.
    \item The statistical tests return a \underline{$p$-value}: the probability
        of obtaining observed performance differences (or more), assuming that
        $H_0$ is correct.
    \item small $p$-value means we can be more confident that one system is
        actually different from another.
    \item performance difference is \underline{statistically significant} if
        $p<0.05$.
    \item $p>0.05$ \emph{does not mean} that the two algorithms are similar.
        E.g. collecting more data can change the $p$-value in certain direction.
    \item \underline{P-hacking} is the misuse of data analysis to find
        patterns in data that can be presented as statistically significant
        when in fact there is no underlying effect. 
    \begin{itemize}
        \item 
            This dramatically increases
            and understates the risk of FPs, i.e.\ reject the $H_0$ where in fact we
            shouldn't, i.e.\ we thought there is difference in distribution, where
            in fact there isn't.
        \item Fight against P-hacking: adaptive $p$-value
            \begin{enumerate}
                \item rank $p$-values from $M$ experiments:
                    \[
                        p_1\le p_2\le p_3\le\ldots\le p_M
                    \]
                \item Calculate the Benjamini-Hochberg critical value for each
                    experiment:
                    \[
                        z_i=0.05 \frac{i}{M}
                    \]
                \item significant results are the ones where the $p$-value is
                    smaller than the critical value
            \end{enumerate} 
    \end{itemize} 
\end{itemize} 

\section{Artificial Neural Networks (ANN)}

\begin{itemize}
    \item ANN: a class of ML algorithms. Architectures of connected neurons,
        usually optimized with gradient descent
    \item Deep learning: using ANN with multiple hidden layers. Complex models
        trained with large datasets.
\end{itemize} 

\subsection{Linear Regression}

\begin{itemize}
    \item Supervised learning
    \item the desired labels are continuous (instead of discrete)
    \item represented as $y = ax + b$
    \item Loss function: 
        \begin{itemize}
            \item 
                ``sum-of-squares'', $E=\frac{1}{2}\sum_{i=1}^{N}
                {\left(\hat{y}^{(i)}-y^{(i)}\right)}^{2}$.
            \item 
                smaller values of $E$ means more accurate predictions
        \end{itemize} 
\end{itemize} 

\subsection{Neurons}

\begin{itemize}
    \item Given input $X$, parameter $\theta$, and \textbf{activation function}
        $g(z)$, we can model the \textbf{artificial neuron} as
        \[
            \hat{y}=g(X\theta).
        \]
    \item \underline{logistic activation function}(logistic function): 
        \[
            g(z)=\frac{1}{1+e^{-z}}.
        \]
    \item \underline{Perceptron}: uses the \underline{threshold function} as the
        activation function
        \[
            h(x) = f(W^Tx) =
            \begin{cases}
                1 & \text{if sum($W^Tx$)} > 0 \\
                0 & \text{otherwise},
            \end{cases} 
        \]
        with the learning rule
        \[
            \theta_i := \theta_i + \alpha(y-h(x))x_i,
        \]
        where $y$ is the desired output and $h(x)$ is the prediction.
    \item connecting multiple neurons in a manner of multiple layers creates a
        \underline{multi-layer network}.
\end{itemize} 

\subsection{Activation functions}

\begin{itemize}
    \item Sigmoid activation:
        \[
            f(x) = \sigma(x) = \frac{1}{1 + e^{-x}}.
        \]
    \item Tanh activation:
        \[
            f(x) = \text{tanh}(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}.
        \]
    \item ReLU activation:
        \[
            f(x)=\text{ReLU}(x)=\begin{cases}
                0 & \text{for } x\le 0 \\
                x & \text{for } x\ge 0.
            \end{cases} 
        \]
    \item Softmax activation:
        \[
            \text{softmax}(z_i)=\frac{e^{z_i}}{\sum_ke^{z_k}}.
        \]
    \item Most activation functions are applied element-wise. Softmax is an
        exception.
    \item ReLU is commonly used for very deep networks. Tanh and sigmoid also
        work well and can be more robust.
    \item \underline{activation of output layer} should depends on the task.
        \begin{itemize}
            \item classifying into two classes $\rightarrow$ sigmoid or tanh
            \item predicting an unbounded score $\rightarrow$ linear
            \item predicting a probability distribution $\rightarrow$ softmax
        \end{itemize} 
\end{itemize} 

\subsection{Loss function}

\begin{itemize}
    \item lower loss means better task performance
    \item MSE (quadratic loss, or L2 loss):
        \[
            \text{MSE}=\frac{1}{N}\sum_{i=1}^{N} {\left(\hat{y}_i-y_i\right)}^{2}
        \]
    \item maximize the likelihood of the network assigning the correct labels to
        all inputs in our dataset:
        \[
            \prod_{i=1}^{N} p\left(y^{(i)}\,|\,x^{(i)};\theta\right),
        \]
        assuming examples are i.i.d. s.t. $p(A\wedge B)=p(A)p(B)$.
    \item For binary classification, the probability of success output given
        input:
        \[
            \prod_{i=1}^{N} {\left(\hat{y}^{(i)}\right)}^{y^{(i)}}
            {\left(1-\hat{y}^{(i)}\right)}^{1-y^{(i)}}.
        \]
        Maximizing the above expression is equivalent to maximizing
        \[
            \sum_{i=1}^{N}
            y^{(i)}\log\left(\hat{y}^{(i)}\right)+(1-\hat{y}^{(i)})\log\left(1-\hat{y}^{(i)}\right).
        \]
        Turning this into a loss we can minimise the \textbf{binary
        cross-entropy}:
        \[
            L=-\frac{1}{N}\sum_{i=1}^{N} 
            y^{(i)}\log\left(\hat{y}^{(i)}\right)+(1-\hat{y}^{(i)})\log\left(1-\hat{y}^{(i)}\right)
        \]
        which can be generalized to \textbf{categorical cross-entropy} for
        multi-class classification:
        \[
            L=-\frac{1}{N}\sum_{i=1}^{N} \sum_{c=1}^{C}
            y_c^{(i)}\log\left(\hat{y}_c^{(i)}\right),
        \]
        where $C$ is the set of possible classes and $\hat{y}_c^{(i)}$ is the
        predicted probability of class $c$ for datapoint $i$.
\end{itemize} 

\subsection{Backward Propagation}

Assume that there are
\begin{itemize}
    \item $N$ samples,
    \item $M$ output nodes,
    \item $D^{(i)}$ nodes at layer $i$, and let the first layer be at index 0,
    \item $t + 1$ number of layers in total.
\end{itemize} 
and let
\begin{itemize}
    \item $L\in\mathbb{R}^{N\times M}$ be the loss of the ANN,
    \item $W^{(i)}\in\mathbb{R}^{D^{(i - 1)}\times D^{(i)}}$ be the weights at layer $i$,
    \item $X^{(i)}\in\mathbb{R}^{N\times D^{(i)}}$
        be the activated node values at layer $i$.
    \item $Z^{(i)}$ be the linear-combination values at layer $i$,
    \item $g(Z)$ be the activation function of $Z$,
    \item $X:=X^{(0)}$ to be the input values
    \item $\hat{Y}:=X^{(t)}$ to be the output values.
\end{itemize} 
Thus
\begin{align*}
    \Delta^{(i)}:=\frac{\partial L}{\partial W^{(i)}}
    & = \frac{\partial Z^{(i)}}{\partial W^{(i)}}\frac{\partial L}{\partial Z^{(i)}}\\
    & = \frac{\partial \left(X^{(i-1)}W^{(i)}\right)}{\partial W^{(i)}}\delta^{(i)}\\
    & = X^{(i-1)}\delta^{(i)},
\end{align*} 
where for $0 < i < t$,
\begin{align*}
    \delta^{(i)}:=\frac{\partial L}{\partial Z^{(i)}}
    & = \frac{\partial Z^{(i+1)}}{\partial Z^{(i)}}
    \frac{\partial L}{\partial Z^{(i + 1)}} \\
    & = \frac{\partial \left(W^{(i+1)}*g(Z^{(i)})\right)}{\partial
    Z^{(i)}}\delta^{(i+1)}\\
    & = \delta^{(i+1)}W^{(i+1)}g'(Z^{(i)}),
\end{align*} 
and for $i = t$,
\begin{align*}
    \delta^{(t)}
    & = \frac{\partial L}{\partial Z^{(t)}} \\
    & = \frac{\partial \hat{Y}}{\partial Z^{(t)}}
    \frac{\partial L}{\partial \hat{Y}}\\
    & = \frac{\partial g(Z^{(t)})}{\partial Z^{(t)}}
    \frac{\partial L}{\partial \hat{Y}}\\
    & = g'(Z^{(t)}) \frac{\partial L}{\partial \hat{Y}},
\end{align*} 
where the value of $\frac{\partial L}{\partial \hat{Y}}$ depends on the chosen
loss function, and the value of $g'(Z^{(i)})$ depends on the chosen activation
function.

\subsection{Gradient Descent}

\begin{itemize}
    \item Definition: Repeatedly update a parameter $W$ by taking small
        steps in the negative direction of the partial derivative, as follow
        \[
            W := W - \alpha \frac{\partial L}{\partial W}
        \]
        where $\alpha$ is a hyperparameter for the learning rate/step size.
    \item vector notation: for a function $f:\mathbb{R}^{K}\mapsto\mathbb{R}$,
        the gradient is
        \[
            \nabla_\theta f(\theta) = \begin{pmatrix}
                \dfrac{\partial f(\theta)}{\partial \theta_1} \\[2ex]
                \dfrac{\partial f(\theta)}{\partial \theta_2} \\
                \vdots \\
                \dfrac{\partial f(\theta)}{\partial \theta_K}
            \end{pmatrix}.
        \]
        In the case of $L(W)$ being MSE, we can find the value of $W$ such that
        the value of $L(W)$ is minimized by setting partial derivative to 0.
        We thus have
        \[
            \nabla_W E(W) = X^T(XW - y)=0
            \quad\Longrightarrow\quad W^* = {\left(X^TX\right)}^{-1}X^Ty,
        \]
        where
        \[
            X = \begin{pmatrix}
                x_1^{(1)} & \ldots & x_{K}^{(1)} & 1.0 \\[1ex]
                x_1^{(2)} & \ldots & x_{K}^{(2)} & 1.0 \\
                \vdots & \ddots & \vdots & \vdots \\
                x_1^{(N)} & \ldots & x_{K}^{(N)} & 1.0
            \end{pmatrix},\qquad
            y = \begin{pmatrix}
                y^{(1)} \\
                y^{(2)} \\
                \vdots \\
                y^{(N)}
            \end{pmatrix},\qquad
            \theta = \begin{pmatrix}
                    a_1 \\
                    \vdots \\
                    a_K \\
                    b
            \end{pmatrix}.
        \]
    \item Comparisons among various gradient descent methods:
        \begin{itemize}
            \item gradient descent: Compute gradient based on the whole dataset.
            \item \underline{stochastic} gradient descent: Loop over each datapoint,
                compute gradient based on the data point and update
                weights. This can be very noisy.
            \item \underline{mini-batched} gradient descent: Loop over batches
                of datapoints, compute gradient based on the batch and update
                weights. This is what we mostly use in practice.
        \end{itemize} 
    \item Adaptive learning rates: Take smaller steps as we get closer to the
        minimum. Strategies for updating the learning rate include:
        \begin{itemize}
            \item every epoch
            \item after a certain number of epochs
            \item when performance on the validation set hasn't improved for
                several epochs.
        \end{itemize} 
    \item Weight initialization:
        \begin{itemize}
            \item zeros
            \item normal: draw randomly from a normal distribution $\sim N(0,1)$ or
                $N(0,0.1)$
            \item Xavier Glorot:
                \[
                    W\sim
                    U\left[-\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}},\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}}\right],
                \]
                where $U$ is the uniform distribution, $n_j$ is the number of
                neurons in the previous layer, $n_{j+1}$ is the number of
                neurons in the next layer.
            \item Random initialization lead to different results, sometimes
                even with the same seed! So embrace randomness, run with
                different random seeds and report the average.
                \begin{itemize}
                    \item GPU threads finish in a random order, leading to
                        randomness.
                    \item Small rounding errors add up.
                \end{itemize} 
        \end{itemize} 
    \item Data normalization:
        \begin{itemize}
            \item min-max normalization: scaling the smallest value to $a$ and
                largest value to $b$,
                \[
                    X' = a +
                    \frac{X-X_\text{min}}{X_\text{max}-X_\text{min}}(b-a)
                \]
            \item standardization (z-normalization): scaling the data to follow
                standard normal distribution,
                \[
                    X'=\frac{X-\mu}{\sigma}
                \]
            \item Normalization helps because $\Delta^{(i)}\propto X^{(i)}$,
                \[
                    \Delta^{(i)}=\frac{\partial L}{\partial
                    W^{(i)}}=X^{(i-1)}\frac{\partial L}{\partial Z^{(i)}}.
                \]
            \item the scaling parameters, like $a$ and $b$ in min-max
                normalization, need to be calculated based on the 
                \emph{training set only}.
        \end{itemize} 
    \item Gradient checking: By calculating
            \[
                W^{(i)}_t = W^{(i)}_{t-1} - \alpha
               \frac{\partial L(W^{(i)})}{\partial W^{(i)}}
               \quad\Longrightarrow\quad
               \frac{\partial L(W^{(i)})}{\partial W^{(i)}}
                =\frac{W^{(i)}_{t-1}-W^{(i)}_t}{\alpha},
            \]
            where $t$ indicates the number of iterations, and
            \[
                \frac{\partial L(W^{(i)})}{\partiaW W^{(i)}}
                \approx \frac{L(W^{(i)} + \epsilon)-L(W^{(i)} -
                \epsilon)}{2\epsilon},
            \]
            where $\epsilon>0$ is very small, both values should be pretty
            similar.
\end{itemize} 

\subsection{Overfitting in ANN}

\begin{itemize}
    \item Very important to use \underline{held-out} validation and test sets.
    \item If ANN is \underline{underfitting} on the training data, we can try
        increasing the number of neurons/layers.
    \item If ANN is \underline{overfitting} on the training data too fast, we
        can try reducing its capacity by lowering the number of neurons/layers.
    \item To tackle the problem of overfitting:
        \begin{itemize}
            \item early stopping:
                We can use the validation data to choose when to stop
                (before the model keeps improving until it overfits).
            \item regularization: Adding some info/constraints to stop the model
                from overfitting.
                \begin{itemize}
                    \item L2 regularization:
                        \[
                            L_2 = L + \lambda \sum_w w^{2}
                            \quad\Longrightarrow\quad
                            W := W - \alpha\left(\frac{\partial L}{\partial
                            W}+2\lambda w\right).
                        \]
                    \item L1 regularization:
                        \[
                            L_1 = L + \lambda \sum_w|w|
                            \;\Longrightarrow\;
                            W := W - \alpha\left(\frac{\partial L}{\partial
                            W}+\lambda\,\text{sign}(w)\right).
                        \]
                \end{itemize} 
            \item dropout: During training, randomly set some (typically 50\%)
                neural activations to 0. During testing, use all the neurons,
                but scale the activations. This prevents the ANN from relying on
                any one node.
        \end{itemize} 
\end{itemize} 

\end{document}
