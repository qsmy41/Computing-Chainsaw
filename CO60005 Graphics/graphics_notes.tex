\documentclass[twocolumn,landscape,10pt]{article}
\usepackage[thinc]{esdiff} % for typesettign derivatives
\usepackage{amsthm} % provides an enhanced version of LaTex's \newtheorem command
\usepackage{mdframed} % framed environments that can split at page boundaries
\usepackage{enumitem} % bulletin points or other means of listing things
\usepackage{amssymb} % for AMS symbols
\usepackage{amsmath} % so as to use align
\usepackage{latexsym} % so as to use symbols like \leadsto
\usepackage{mathrsfs} % for using mathscr for char like operators
\usepackage{commath} % for using norm symbol
\usepackage{mathtools} % for using environments like dcases
\usepackage{authblk} % for writing affiliations
\usepackage{graphicx} % for importing images
\graphicspath{{./images/}} % for the path to images, also always put label behind captions
\usepackage{subfig}
\usepackage{textcomp} % for using degree symbol
\usepackage{hyperref} % for clickable link in the pdf & customizable reference text
\usepackage[all]{hypcap} % for clickable link to images instead of caption
\usepackage[margin=1.0in]{geometry} % default is 1.5in
% \usepackage[left=0.4in, right=0.4in, top=0.8in, bottom=0.8in]{geometry}
\usepackage[title]{appendix} % for attaching appendix
\allowdisplaybreaks % allow page breaking in display maths, like align
\usepackage{xcolor} % for setting color of a block of text, use \textcolor{<color>}{}
\usepackage[normalem]{ulem} % for strikethrough text, use \sout{}
% allow for more advanced table layout
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
% for adjusting caption settings
\usepackage[justification=centering]{caption}
\captionsetup[table]{skip=10pt}

\theoremstyle{definition}
\mdfdefinestyle{defEnv}{%
  hidealllines=false,
  nobreak=true,
  innertopmargin=-1ex,
}

% The following is for writing block of code
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% setting of the thickness of the 4 lines of box
\setlength{\fboxrule}{2pt}

% Use the following to change code language and related settings
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  literate={~} {$\sim$}{1}
}

\pagestyle{headings}
\author{Lectured by Bernhard Kainz and Abhijeet Ghosh}
\title{Graphics}
\affil{Typed by Aris Zhu Yi Qing}
\begin{document}
\maketitle
\tableofcontents

\newpage
\section{Projections and Transformations}

\subsection{Parallel Projection}

\begin{itemize}
    \item For a vertex $\mathbf{V} = (V_x,V_y, V_z)^T$, the
        \underline{\textbf{projector}}
        (projection line) is defined by the parametric line equation
        \[
            \mathbf{P}=\mathbf{V}+\mu \mathbf{d}
        \]
    \item Assuming the projection plane is $z=0$, we can establish
        \[
            0 = P_z = V_z + \mu d_z
        \]
        to obtain $\mu$, thereby computing $P_x$ and $P_y$.
    \item \underline{\textbf{Orthographic projection}} is a special type of
        parallel projection:
        \begin{itemize}
            \item projection plane: $z=0$
            \item $\mathbf{d}=\begin{pmatrix}
                    0 & 0 & -1
                \end{pmatrix}^T$
            \item  $P_x=V_x$, $P_y=V_y$
        \end{itemize} 
\end{itemize} 

\subsection{Perspective Projections}

\begin{itemize}
    \item The \underline{\textbf{centre of projection}} is the viewpoint, which
        all the projectors pass through, assumed to be at the origin.
    \item For a vertex $\mathbf{V} = (V_x,V_y, V_z)^T$, the projector
        $\mathbf{P}$ has the equation
        \[
            \mathbf{P}=\mu\mathbf{V}
        \]
    \item Since the projection plane is at a constant $z$ value $f$, at the
        point of intersection we have
        \[
            f = P_z = \mu V_z
        \]
        to obtain $\mu$, thereby computing $P_x$ and $P_y$.
\end{itemize} 

\subsection{Space Transformations}

\subsubsection{Homogeneous Coordinates}

\begin{itemize}
    \item A \underline{\textbf{homogeneous coordinate}} is a three-dimensional
        coordinate with a fourth componenet called
        \underline{\textbf{ordinate}} which acts as a scale factor.
    \item Assuming a point $\mathbf{P}=(p_x,p_y,p_z)$ 
        in Cartesian coordinate, we introduce $s$ being the ordinate
        \[
            \mathbf{P}'=(p_x, p_y, p_z, s)
        \]
        to form a homogeneous coordinate.
    \item To convert $\mathbf{P}'$ back ot Cartesian, we will perform
        \underline{\textbf{perspective division}}
        \[
            \mathbf{P}''=\left(\frac{p_x}{s},\frac{p_y}{s},\frac{p_z}{s}\right),
        \]
        i.e.\ divide $x$, $y$ and $z$ values by the ordinate.
        Thus when $s=1$, $\mathbf{P}=\mathbf{P}''$.
    \item If $s\neq 0$, we have a \underline{\textbf{position vector}}.
        If $s=0$, we have a \underline{\textbf{direction vector}}.
\end{itemize} 

\subsubsection{Translation Matrix}

To apply a translation vector $\mathbf{t}=(t_x,t_y,t_z)$ to a point
$\mathbf{P}=(p_x,p_y,p_z)$, we do
\[
    \begin{pmatrix}
        1 & 0 & 0 & t_x \\
        0 & 1 & 0 & t_y \\
        0 & 0 & 1 & t_z \\
        0 & 0 & 0 & 1
    \end{pmatrix} 
    \begin{pmatrix}
        p_x \\
        p_y \\
        p_z \\
        1
    \end{pmatrix} 
    =
    \begin{pmatrix}
        p_x + t_x \\
        p_y + t_y \\
        p_z + t_z \\
        1
    \end{pmatrix} 
\]
with the inverse of the translation matrix as
\[
    \begin{pmatrix}
        1 & 0 & 0 & -t_x \\
        0 & 1 & 0 & -t_y \\
        0 & 0 & 1 & -t_z \\
        0 & 0 & 0 & 1
    \end{pmatrix}.
\]

\subsubsection{Scaling Matrix}

To scale a point from the origin, we can do
\[
    \begin{pmatrix}
        s_x & 0 & 0 & 0 \\
        0 & s_y & 0 & 0 \\
        0 & 0 & s_z & 0 \\
        0 & 0 & 0 & 1 \\
    \end{pmatrix} 
    \begin{pmatrix}
        p_x \\
        p_y \\
        p_z \\
        1
    \end{pmatrix} 
    =
    \begin{pmatrix}
        s_xp_x \\
        s_yp_y \\
        s_zp_z \\
        1
    \end{pmatrix} 
\]
with the inverse of the scaling matrix as
\[
    \begin{pmatrix}
        1/s_x & 0 & 0 & 0 \\
        0 & 1/s_y & 0 & 0 \\
        0 & 0 & 1/s_z & 0 \\
        0 & 0 & 0 & 1 \\
    \end{pmatrix} 
\]

\subsubsection{Rotation Matrix}

To rotate \underline{anti-clockwise} when looking \underline{along the direction} 
of the axis with a \underline{left-hand} axis system, we have
\[
    \mathcal{R}_x=
    \begin{pmatrix}
        1 & 0 & 0 & 0 \\
        0 & \cos{\theta} & -\sin{\theta} & 0 \\
        0 & \sin{\theta} & \cos{\theta} & 0 \\
        0 & 0 & 0 & 1
    \end{pmatrix}, \qquad
    \mathcal{R}_x^{-1}=
    \begin{pmatrix}
        1 & 0 & 0 & 0 \\
        0 & \cos{\theta} & \sin{\theta} & 0 \\
        0 & -\sin{\theta} & \cos{\theta} & 0 \\
        0 & 0 & 0 & 1
    \end{pmatrix},
\]
\[
    \mathcal{R}_y=
    \begin{pmatrix}
        \cos{\theta} & 0 & \sin{\theta} & 0 \\
        0 & 1 & 0 & 0 \\
        -\sin{\theta} & 0 & \cos{\theta} & 0 \\
        0 & 0 & 0 & 1
    \end{pmatrix}, \qquad
    \mathcal{R}_y^{-1}=
    \begin{pmatrix}
        \cos{\theta} & 0 & -\sin{\theta} & 0 \\
        0 & 1 & 0 & 0 \\
        \sin{\theta} & 0 & \cos{\theta} & 0 \\
        0 & 0 & 0 & 1
    \end{pmatrix}, 
\]
\[
    \mathcal{R}_z=
    \begin{pmatrix}
        \cos{\theta} & -\sin{\theta} & 0 & 0 \\
        \sin{\theta} & \cos{\theta} & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1
    \end{pmatrix} \qquad
    \mathcal{R}_z^{-1}=
    \begin{pmatrix}
        \cos{\theta} & \sin{\theta} & 0 & 0 \\
        -\sin{\theta} & \cos{\theta} & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1
    \end{pmatrix},
\]
to rotate along $x$, $y$ and $z$ axis respectively.
In other words, the right matrices are rotating clockwise.

\subsubsection{Projection Matrix}

For a perspective projection, placing the centre of projection at the origin and
using $z=f$ as before, we can use
\[
    \mathcal{M}_p=
    \begin{pmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 1/f & 0 \\
    \end{pmatrix} 
\]
For an orthographic projection, with the projection plane at $z=0$, we can use
\[
    \mathcal{M}_o=
    \begin{pmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 
    \end{pmatrix} 
\]

\section{Clipping}

\begin{itemize}
    \item \underline{\textbf{Clipping}} eliminates portions of objects outside
        the \underline{\textbf{viewing frustum}}, which is the boundaries of the
        image plane projected in 3D with a near and far clipping plane.
    \item \underline{Why} clipping?
        \begin{itemize}
            \item avoid degeneracy: e.g. don't draw objects behind the camera
            \item improve efficiency: e.g. do not process objects which are not visible.
        \end{itemize} 
    \item \underline{When} to clip?
        \begin{itemize}
            \item before perspective transform in 3D space:
                \begin{itemize}
                    \item 3D world space
                    \item use the equation of 6 planes
                    \item natural, not too degenerate
                \end{itemize} 
            \item in homogeneous coordinates after perspective transform 
                and \underline{before} perspective division:
                \begin{itemize}
                    \item clip space
                    \item canonical, independent of camera
                    \item simplest to implement, since clipping plane can align
                        with axis so that we can easily discard anything further
                        than the far plane or closer than the near plane
                \end{itemize} 
            \item in the transformed 3D screen space \underline{after} 
                perspective division:
                \begin{itemize}
                    \item Normalized Device Coordinates (NDC)
                    \item The regions extends from -1. to 1. in each axis.
                        Anything outside from the volume is discarded.
                    \item problem --- having negative orginates 
                \end{itemize} 
        \end{itemize} 
    \item \underline{\textbf{Halfspace}} We can define any plane as a test for a
        point $\mathbf{p}$:
        \[
            f(x,y,z)=\mathbf{H}\cdot \mathbf{p}=0
        \]
        where $\mathbf{H}=(H_x,H_y,H_z,H_s)$ and $\mathbf{p}=(x,y,z,1)$,
        such that 
        \[
            \begin{cases}
                \mathbf{H}\cdot \mathbf{p}>0  & \text{in one halfspace (pass-through)}\\
                \mathbf{H}\cdot \mathbf{p}<0 & \text{in the other halfspace (clip/cull/reject)}
            \end{cases} 
        \]
        \begin{itemize}
            \item \underline{\textbf{Segment Clipping}} Similarly we have
                \[
                    \begin{cases}
                        \mathbf{H}\cdot\mathbf{p}>0, \mathbf{H}\cdot\mathbf{q}<0
                        & \text{clip $\mathbf{q}$ to plane} \\
                        \mathbf{H}\cdot\mathbf{p}<0, \mathbf{H}\cdot\mathbf{q}>0
                        & \text{clip $\mathbf{p}$ to plane} \\
                        \mathbf{H}\cdot\mathbf{p}>0, \mathbf{H}\cdot\mathbf{q}>0
                        & \text{pass through} \\
                        \mathbf{H}\cdot\mathbf{p}<0, \mathbf{H}\cdot\mathbf{q}<0
                        & \text{clipped out}
                    \end{cases} 
                \]
            \item Test if an object is convex.
                \begin{enumerate}
                    \item For each face of the object, pick a random point.
                    \item For this point, compare with points from other faces,
                        check if
                        \[
                            \texttt{sign(f(xj,yj,zj)) != sign(f(xi,yi,zi))}
                        \]
                        then it is not convex.
                \end{enumerate} 
            \item Test if a point is contained in a concave object.
                \begin{itemize}
                    \item Cast a ray from the test point in any direction. If
                        the number of intersections with the object is odd, then
                        the test point is inside.
                \end{itemize} 
        \end{itemize} 
\end{itemize} 

\section{Graphics Pipeline}

\subsection{Application}

\begin{itemize}
    \item executed by the software on the main processor (CPU)
    \item typical tasks performed: collision detection, animation, morphing,
        perform spatial subdivision scheme (quadtree, octree).
    \item to reduce the amount of main memory required at a given time
\end{itemize} 

\subsection{Geometry}

\begin{enumerate}
    \item Modelling Transformations
    \item Illumination (Shading)
    \item Viewing Transformations (Perspective/Orthographic)
    \item Clipping
    \item Projection (to screen space --- window-viewport transformation)
\end{enumerate} 

\subsection{Rasterization}

\begin{itemize}
    \item \underline{\textbf{Rasterization}} is the task of taking an image 
        described in a vector graphics format (shapes) and 
        converting it into a raster image (a series of pixels).
    \item During this process, fragments/raster points are created from 
        continuous primitives.  A \underline{\textbf{fragment}} 
        can be thought of as the data needed to shade the pixel 
        (e.g. color, illumination, texture) 
        and to test whether the fragment survives to become a pixel 
        (depth, alpha, etc.)
    \item Eventually, one or more fragments are merged to become a
        \underline{\textbf{pixel}}, which is the smallest addressable element
        in a raster image.
    \item To prevent from exposing the process of gradual screening of the
        primitives, double buffering is used so that 
        the rasterization takes place in a special memory,
        and as soon as the image is completely rastered, 
        it is copied into the visible area of the image memory (frame buffer).
\end{itemize} 

\subsection{Shading}
\underline{\textbf{Shading}} refers to the modification of individual 
vertices or fragments within the graphics pipeline.
This is the \emph{programmable} part of the graphics pipeline.

\subsubsection{Vertex Shader}
\begin{itemize}
    \item executed once for each vertex
    \item only has access to the vertex and no neighbouring vertices,
        the topology, or similar
\end{itemize} 

\subsubsection{Tessellation Shader}
\begin{itemize}
    \item divides an area (triangle or square) into smaller areas
    \item advantage: allow detail to be dynamically added and subtracted
        from a 3D polygon mesh and its silhouette edges based on control
        parameters (e.g. camera distance)
    \item The \underline{\emph{Tessellation Control Shader}} (TCS) determines
        how much tessellation to do. It is optional; default
        tessellation values can be used.
    \item The \underline{\emph{tessellation primitive generator}} 
        (not programmable) takes the input patch 
        and subdivides it based on values computed by the TCS.
    \item The \underline{\emph{Tessellation Evaluation Shader}} (TES) takes the
        tessellated patch and computes the vertex values for each
        generated vertex.
\end{itemize} 

\subsubsection{Geometry Shader}
\begin{itemize}
    \item takes a single primitive as input and may output zero or more
        primitives of the same type
    \item has access to multiple vertices, if the primitive consists of multiple
        vertices
    \item A \underline{\textbf{primitive}} can mean
        \begin{enumerate}[label = (\alph*)]
            \item the interpretation scheme to determine what a stream
                of vertices represents when being rendered,
                which can be arbitrarily long
            \item the \emph{result} of the interpretation of a vertex
                stream (also called the \underline{\emph{base primitive}})
        \end{enumerate} 
    \item Use case: in a particle system, the inputs are processed
        points, and geometry shader generates polygons/cubes/etc. to
        save computation in the previous pipelines.
\end{itemize} 

\subsubsection{Fragment Shader}

\begin{itemize}
    \item executed once for every fragment generated by the rasterization
    \item it takes in interpolated vertex attributes
    \item it calculates the color of the corresponding fragment
\end{itemize} 

\section{OpenGL}

\begin{itemize}
    \item The interface is platform independent, but the implementation is
        platform dependent.
    \item It defines an abstract rendering device and a set of functions to
        operate the device.
    \item It is a low-level ``immediate mode'' graphics API with drawing
        commands and no concept of permanent objects, operating as a state
        machine.
    \item To write an OpenGL programme, we need to
        \begin{enumerate}
            \item create a render window via library such as glut, Qt, etc.
            \item setup viewport, model transformation and file I/O (shader,
                textures, etc.)
            \item implement frame-generataion (update/rendering) functions 
                to define what happens in every frame
        \end{enumerate} 
    \item Basic concepts:
        \begin{itemize}
            \item \underline{\textbf{Context}}
                \begin{itemize}
                    \item represents an instance of OpenGL
                    \item a process can have multiple contexts to share
                        resources
                    \item one-to-one mapping between a context and a thread
                \end{itemize} 
            \item \underline{\textbf{Resources}}
                \begin{itemize}
                    \item act as sources of input and sinks for output
                    \item e.g. texture images(input), buffers(output)
                \end{itemize} 
            \item \underline{\textbf{Object Model}}
                \begin{itemize}
                    \item Object instances are identified by unique names
                        (unsigned integer handle).
                    \item Commands work on targets, where each target is bounded
                        by an object.
                \end{itemize} 
        \end{itemize} 
    \item \underline{\textbf{Buffer objects}} are regular OpenGL objects that
        store an array of unformatted memory allocated by the OpenGL context
        (i.e.\ GPU).
    \item has primitive types such as \texttt{GL\_POINTS}, \texttt{GL\_LINES},
        \texttt{GL\_POLYGONS}, \texttt{GL\_TRIANGLES}, etc.
\end{itemize} 

\section{Illumination and Shading}

\subsection{Physics of Shading}

\begin{itemize}
    \item object properties
        \begin{itemize}
            \item the position of the object relative to the light sources
            \item the surface normal vector
            \item the albedo of the surface (ability to absorb light energy) and
                the reflectivity of the surface
        \end{itemize} 
    \item light source properties
        \begin{itemize}
            \item intensity of the emitted light
            \item distance to the point on the surface
        \end{itemize} 
    \item energy (Joule) of a photon is
        \[
            e(\lambda) = \frac{hc}{\lambda}
        \]
        where $h\approx 6.63\times 10^{-34}J\cdot s$ and $c\approx 3\times
        10^{8}m/s$.
    \item radiant energy (Joule) of $n$ photons is
        \[
            Q = \sum_{i=1}^{n} e(\lambda_i)
        \]
    \item Radiation/radiant/electromagnetic flux (Watts) is
        \[
            \Phi=\frac{\mathrm{d} Q}{\mathrm{d}t}
        \]
    \item \underline{\textbf{Radiance}} (Watt/($\text{meter}^{2}\,\cdot$ steradian)) 
        is density of a incident flux falling onto a surface 
        \underline{in a particular direction}
        \[
            L(\omega)=\frac{\mathrm{d}^2\Phi}{\cos{\theta}\;\mathrm{d}A\mathrm{d}\omega}
        \]
    \item \underline{\textbf{Irradiance}} (Watt/$\text{meter}^{2}$) 
        is density of the incident flux falling onto a surface
        \[
            E=\frac{\mathrm{d}\Phi}{\mathrm{d}A}
        \]
    \item  We define the
        \underline{\textbf{Bidirectional Reflectance Distribution Function}}
        (BRDF) (1/steradian)
        \[
            f_r(\theta_i,\phi_i,\theta_r,\phi_r)=
            f_r(\omega_i,\omega_r)=\frac{\mathrm{d}L_r(\omega_r)}{\mathrm{d}E_i(\omega_i)}
        \]
        by ignoring other physical phenomenon such as absorption,
        transmission, fluorescence, diffraction, etc.
        \begin{itemize}
            \item \underline{\emph{Isotropic BRDF}} is such that rotation along
                surface normal does not change reflectance.
            \item \underline{\emph{Anisotropic BRDF}} changes reflectance when
                rotating along surface normal, which happens on surfaces with
                strongly oriented microgeometry elements such as brushed metals,
                hair, cloth, etc.
            \item non-negativity: $f_r(\omega_i,\omega_r)\ge 0$
            \item energy conservation: $\forall\omega_i, \int_\Omega
                f_r(\omega_i,\omega_r)\cos{\theta_r}\mathrm{d}\omega_r\le 1$
            \item reciprocity: $f_r(\omega_i,\omega_r)=f_r(\omega_r,\omega_i)$
        \end{itemize} 
    \item To compute the reflected radiance discretely, with $n$ points light
        sources, we have
        \[
            L_r(\omega_r)=\sum_{i=1}^{n} f_r(\omega_i,\omega_r)E_i
            =\sum_{i=1}^{n}
            f_r(\omega_i,\omega_r)\cos{\theta_i}\frac{\Phi_i}{4\pi d_i^2}
        \]
    \item Ideally, BRDF is constant, so with a single point light source
        \[
            L(\omega_r)=k_d(n\cdot l) \frac{\Phi_s}{4\pi d^2}
        \]
        where $k_d$ is the diffuse reflection coefficient,
        $n$ is the (normalized) surface normal,
        and $l$ is the (normalized) light direction from surface.
\end{itemize} 

\subsection{The Phong Model}

\begin{itemize}
    \item light sources are assumed to be point-shaped, i.e.\ no spatial extent
    \item Reflected radiance calculation is
        \[
            L(\omega_r) = k_s{(v\cdot r)}^{q}\frac{\Phi_s}{4\pi d^2}
            = k_s{(v\cdot (2(n\cdot l)n - l))}^{q}\frac{\Phi_s}{4\pi d^2}
        \]
        where $k_s$ is the specular reflection coefficient, $q$ is the specular
        reflection exponent, $v$ is the direction vector from surface to camera,
        and $r$ is the reflected ray.
    \item Blinn-Phong variation is that
        \[
            L(\omega_r) = k_s{(n\cdot h)}^{q}\frac{\Phi_s}{4\pi d^2}
            \qquad\text{with}\qquad
            h=\frac{l+v}{\norm{l+v}}
        \]
    \item The Phong model is the sum of three components: diffuse, specular and
        ambient, i.e.\
        \[
            L(\omega_r)=k_a + \left(k_d(n\cdot l)+k_s{(v\cdot r)}^{q}\right)
            \frac{\Phi_s}{4\pi d^2}
        \]
    \item Sometimes using $(d+s)$ instead of $d^2$ produces better result, where
        $s$ is a heuristic constant.
\end{itemize} 

\subsection{Shading}

\subsubsection{Flat Shading}

\begin{itemize}
    \item each polygon is shaded uniformly over its surface
    \item computed by taking a point in the center and at the surface normal
    \item normally only the diffuse and ambient components are used
\end{itemize} 

\subsubsection{Gouraud Shading}

\begin{itemize}
    \item interpolate color using shade value at each vertex
    \item can interpolate intensity at each vertex from all the polygons that
        meet at that vertex to create the impression of a smooth surface
    \item cannot accurately model specular components,
        since we don't have normal vector at each point on a polygon
\end{itemize} 

\subsubsection{Phong Shading}

\begin{itemize}
    \item interpolate normals across triangles at fragment stage
    \item more accurate modelling of specular components, but slower
\end{itemize} 


\section{Color}

\subsection{RGB CIE Color Space}

\begin{itemize}
    \item a standard normalized representation of colors
        ranging from 0 to 1, with
        \[
            x=\frac{r}{r+g+b},y=\frac{g}{r+g+b},z=\frac{b}{r+g+b}=1-x-y
        \]
    \item the actual visible colors are a subset of this as shown in Figure
        \ref{fig:cie},
        done through manual testing
        \begin{figure}
          	\includegraphics[scale=0.20]{cie.png}
          	\centering
          	\caption{The CIE 1931 color space. Red is at $(0.628,0.346,0.026)$,
            Green is at $(0.268, 0.588,0.144)$, Blue is at $(0.150,0.07,0.780)$}\label{fig:cie}
        \end{figure}
        
    \item the shape must be convex, since any blend (interpolation) of pure
        colors should create a color in the visible region.
    \item the \underline{\textbf{pure colors}} are around the edge of the
        diagram, also called \underline{\textbf{fully saturated}}
    \item the line joining purple and red has no pure equivalent; the colours
        can only be created by blending
    \item \underline{\textbf{Saturation}} of an arbitrary point is the ratio of
        its distance to the white point over the distance of the white point to
        the edge.
    \item white point: when $x=y=z=0.\dot{3}$
    \item The \underline{\textbf{complement color}} of a color is the point
        diametrically opposite through the white point. Computationally, if the
        color has value $(r,g,b)$, its complement color is
        $(255-r,255-g,255-b)$.
    \item The \underline{\textbf{additive primaries}} are RGB (Red, Green, Blue)
        and the \underline{\textbf{subtractive primaries}} are CMY (Cyan,
        Magenta, Yellow).
        Red is the complement color of Cyan, and similarly for Green and Blue,
        as shown in Figure \ref{fig:primaries}.
        \begin{figure}
          	\includegraphics[scale=0.55]{primaries.png}
          	\centering
          	\caption{additive and subtractive primaries}\label{fig:primaries}
        \end{figure}
    \item RGB can be converted to CIE by
        \[
            \begin{pmatrix}
                x \\
                y \\
                z
            \end{pmatrix} 
            =
            \begin{pmatrix}
                0.628 & 0.268 & 0.15 \\
                0.346 & 0.588 & 0.07 \\
                0.026 & 0.144 & 0.78
            \end{pmatrix} 
            \begin{pmatrix}
                r \\
                g \\
                b
            \end{pmatrix} 
        \]
\end{itemize} 

\subsection{HSV Color Representation}

\begin{itemize}
    \item \underline{\textbf{Hue}} corresponds notionally to pure color
    \item \underline{\textbf{Saturation}} is the proportion of pure color
    \item \underline{\textbf{Value}} is the brightness/intensity
    \item We can visualize the perceptual color space in HSV as in Figure
        \ref{fig:hsv}.
        \begin{figure}
            \centering
            \subfloat[][The conical representation of the HSV model]{ 
                \includegraphics[scale=0.55]{hsv1.png}
            }
            \subfloat[][The HSV color wheel]{ 
                \includegraphics[scale=0.55]{hsv2.png}
            }
          	\caption{HSV model}\label{fig:hsv}
        \end{figure}
    \item Conversion between RGB and HSV can be done as
        \begin{align*}
            V & = \text{max}(r, g, b) \\
            S & = \frac{\text{max}(r,g,b)-\text{min}(r,g,b)}{\text{max}(r,g,b)}\\
            H & = 
            \begin{cases}
                \text{undefined} & r=g=b \\
                120\cdot \dfrac{g-b}{(r-b)+(g-b)} & (r>b)\wedge(g>b) \\[8px]
                120 + 120\cdot \dfrac{b-r}{(g-r)+(b-r)} & (g>r)\wedge(b>r) \\[8px]
                240 + 120\cdot \dfrac{r-g}{(r-g)+(b-g)} & (r>g)\wedge(b>g)
            \end{cases} 
        \end{align*} 
\end{itemize} 

\subsection{Transparancy}

We can model transparency with an $\alpha$ channel, with
\begin{itemize}
    \item transparent: $\alpha=0$
    \item semi-transparent: $0<\alpha<1$
    \item opaque: $\alpha=1$
\end{itemize} 
Suppose that we put $A$ over $B$ over background $G$,
\begin{itemize}
    \item How much of $B$ is blocked by $A$? $\alpha_A$
    \item How much of $B$ shows through $A$? $(1-\alpha_A)$
    \item How much of $G$ shows through both $A$ and $B$? $(1-\alpha_A)(1-\alpha_B)$
    \item How much does $G$ contribute to the overall color?
        $(1-\alpha_A)(1-\alpha_B)\alpha_G$
\end{itemize} 


\section{Texture}

\subsection{Definition}

\begin{itemize}
    \item \underline{\textbf{Texture (map)}} is an image applied (mapped)
        to the surface of a shape or polygon.
    \item A texture can be 1D, 2D, or 3D, but 2D is the most common for visible
        surfaces.
    \item \underline{\textbf{Raster images}} are 2D rectangular matrices or grid
        of square pixels, often used as textures.
    \item \underline{\textbf{Procedural texture}} is a texture created using a
        mathematical description rather than directly stored data 
        (e.g. raster image). Mathematically, it is a function $f$ defiend as
        \[
            f:\mathbf{p}\mapsto\text{color},
        \]
        where $\mathbf{p}$ is a coordinate.
        \begin{itemize}
            \item[+] Very small memory footprint before the texture map is
                generated. The ultimate way in image compression.
            \item[+] No texture memory is really needed since generated ``on the
                fly'' in a fragment shader, resulting in the exactly right level
                of detail for each pixel on the screen.
            \item[-] Hard to get a formula to get the exact/natural look.
            \item[-] On-the-fly generation can take a lot of shader
                program instructions, almost always slower than just
                loading/looking up one.
        \end{itemize} 
\end{itemize} 

\subsection{Photo Textures Mapping}

\subsubsection{Mechanism}

\begin{itemize}
    \item Define a 2D coordinate system on an image mapped onto a 3D object.
    \item For each fragment on an object's surface, work out what coordinate 
        needs to be sampled in the image's 2D space to get the right color.
    \item Conventionally, texture coordinates are denoted with $(s,t)$ 
        (\emph{texture space}). Canonically it goes from (0,0) to (0,1).
        The object surface is denoted with $(u,v)$ (\emph{object space}) 
        and the pixel on the screen is denoted with $(x,y)$ (\emph{screen space}).
        We need to define
        \[
            \text{\underline{\textbf{Parameterization}}}:(s,t)\mapsto(u,v)
        \]
        which is the process of finding parametric equations of textures and
        objects so that texture can be mapped onto object surface, and
        \[
            \text{\underline{\textbf{Rendering}}}:(u,v)\mapsto(x,y)
        \]
        which is the process of generating an image from a model.
\end{itemize} 

\subsubsection{Parameterization}

\begin{itemize}
    \item \underline{Planar mapping}: ignore one of the coordinates
    \item \underline{Cylindrical/Spherical mapping}: compare to
        cylindrical/spherical coordinate systems
    \item \underline{Box mapping}: 6 planar mapping
    \item \underline{\textbf{Unwrapping}}: the process of creating manual
        mapping
\end{itemize} 

\subsubsection{Texture Addressing (Mode)}

What happens outside $[0,1]$?
Following the order in Figure \ref{fig:texture_addressing},
\begin{itemize}
    \item \underline{static color}: we use a static color
        (red in this case)
    \item \underline{clamped}: we use the last color in the range
    \item \underline{repeated}: wrap back to the first coordinate, repeating the
        texture
    \item \underline{mirrored}: similar to repeated, but go backwards insteads
        after ending the texture
\end{itemize} 

\begin{figure}
  	\includegraphics[scale=0.30]{texture_addressing.png}
  	\centering
  	\caption{Different texture addressing}\label{fig:texture_addressing}
\end{figure}


\subsection{Perspective Correct Interpolation}

\begin{itemize}
    \item cannot simply perform linear interpolation as in Gouraud shading on
        texture coordinates
    \item The problem:
        \begin{itemize}
            \item perspective projection \underline{\textbf{does not preserve}}
                linear combinations of points
            \item e.g. equal distances in 3D do not
                map to equal distances in screen space, as shown in figure
                \ref{fig:inter_err}.
        \end{itemize} 
        \begin{figure}
          	\includegraphics[scale=0.55]{interpolation_error.png}
          	\centering
          	\caption{perspective interpolation error}\label{fig:inter_err}
        \end{figure}
    \item The solution: 
        \begin{itemize}
            \item Assume image plane is at $z=f=1$.
            \item Let $t$ controls linear blend of texture coordinates of
                $\mathbf{p}$ and $\mathbf{r}$ and let 
                \[
                    t_p=0,\; t_r=1.
                \]
            \item  Let $\text{lerp}(x,y)$
                be the linear interpolation function, and as such
                \[
                    t_q=t_{q'}z_q=
                    \frac{\text{lerp}(t_{p'},t_{r'})}{\frac{1}{z_q}}=
                    \frac{\text{lerp}(\frac{t_p}{z_p},\frac{t_r}{z_r})}{\text{lerp}(\frac{1}{z_p},\frac{1}{z_r})}
                \]
        \end{itemize} 
    \item The algorithm: given texture parameter $t$ at vertices,
        \begin{enumerate}
            \item compute $\frac{1}{z}$ for each vertex
            \item linearly interpolate $\frac{1}{z}$ across the triangle
            \item linearly interpolate $\frac{t}{z}$ across the triangle
            \item perform perspective division via dividing $\frac{t}{z}$ by
                $\frac{1}{z}$ to obtain $t$
        \end{enumerate} 
\end{itemize} 

\subsection{Texture Mapping and Illumination}

\begin{itemize}
    \item For texture-mapped object, changing the lighting will not show the
        unevenness of the object's surface.
    \item \underline{\textbf{Bump mapping}}: textures to alter the surface
        normal of an object. Shading on the object is changed, but its
        silhouette is not.
    \item \underline{\textbf{Displacement mapping}}: textures to change the
        shading of both the object and its silhouette. It actually moves the
        surface point --- geometry is displaced before determining visibility.
    \item \underline{\textbf{Environment mapping}}: we can simulate reflections
        by using the direction of the reflected ray to index a spherical texture
        map at ``infinity''.
\end{itemize} 

\section{Rasterization, Visibility, and Anti-aliasing}

\subsection{Background}

\begin{itemize}
    \item \underline{\textbf{Rasterization}} determines which pixels are drawn
        into the framebuffer.
    \item Pixels have unique framebuffer location, but multiple fragments can be
        at the same address.
    \item \underline{\textbf{Alias effects}}: an unreal visual artefacts 
        caused by undersampling e.g. straight lines look jagged.
\end{itemize} 

\subsection{Barycentric Coordinates}

\begin{itemize}
    \item let vertices of a triangle be $\mathbf{a}$, $\mathbf{b}$, and $\mathbf{c}$,
        then any point $\mathbf{p}$ can be specified in the plane as
        \begin{align*}
            \mathbf{p}
            &=\mathbf{a}+\beta(\mathbf{b}-\mathbf{a})+\gamma(\mathbf{c}-\mathbf{a})
            =(1-\beta-\gamma)\mathbf{a}+\beta \mathbf{b}+\gamma \mathbf{c} \\
            &=\alpha \mathbf{a}+\beta \mathbf{b}+\gamma \mathbf{c},
        \end{align*} 
        and $(\alpha,\beta,\gamma)$ is called (absolute) barycentric coordinates.
    \item If $\mathbf{p}$ is inside the triangle
        $(\mathbf{a},\mathbf{b},\mathbf{c})$, we have $0<\alpha,\beta,\gamma<1$;
        if $\mathbf{p}$ is on an edge, one coefficient is 0;
        if $\mathbf{p}$ is on a vertex, one coefficient is 1.
    \item Since implicit equation in 2D is defined as
        \[
            f(x,y)=Ax+By+c=0
        \]
        and an implicit line through $(x_a,y_a)$ and $(x_b,y_b)$ is
        \[
            f_{ab}(x,y)=(y_a-y_b)x+(x_b-x_a)y+x_ay_b-x_by_a=0,
        \]
        note that a barycentric coordinate such as $\beta$ is a
        \underline{\textbf{signed distance}} from a line 
        (in this case, the line through $\mathbf{a}$ and $\mathbf{c}$),
        and we can use implicit line equations to evaluate the signed distance,
        we have
        \[
            \frac{f_{ac}(x_b,y_b)}{f_{ac}(x,y)}=\frac{1}{\beta}
            \Longrightarrow
            \beta=\frac{f_{ac}(x,y)}{f_{ac}(x_b,y_b)}
        \]
        and we can obtain similar results for $\alpha$ and $\gamma$, thereby
        computing the barycentric coordinates with the given cartesian coordinates.
    \item In general, the barycentric coordinates for $\mathbf{p}$ are the
        solution of the linear system
        \[
            \begin{pmatrix}
                x_a & x_b & x_c \\
                y_a & y_b & y_c \\
                1 & 1 & 1
            \end{pmatrix} 
            \begin{pmatrix}
                \alpha \\
                \beta \\
                \gamma
            \end{pmatrix} 
            =
            \begin{pmatrix}
                x \\
                y \\
                1
            \end{pmatrix} 
        \]
    \item Let $a,b,c$ be the side lengths of the triangle,
        from barycentric to trilinear coordinates:
        \[
            (\alpha,\beta,\gamma)\mapsto(\frac{\alpha}{a},\frac{\beta}{b},\frac{\gamma}{c}),
        \]
        from trilinear to barycentric coordinates:
        \[
            (t_1,t_2,t_3)\mapsto(t_1a,t_2b,t_3c)
        \]
    \item \underline{\textbf{Triangle Rasterization}}:
        we can check if ($0<\alpha,\beta,\gamma<1$) to determine if
        a fragment of a triangle should be generated.
\end{itemize} 

\subsection{Visibility}

\begin{itemize}
    \item \underline{\textbf{Painter's algorithm}}: sort the triangles using the
        $z$ values in camera space and draw them from back to front.
        \begin{itemize}
            \item[-] not efficient due to costly sorting
            \item[-] suffer from correctness issues such as intersections,
                cycles, etc.
        \end{itemize} 
    \item \underline{\textbf{depth buffer (z-buffer)}}: perform hidden surface
        removal per-fragment by saving the $z$ value for each fragment and keep
        only the fragment with the smallest $z$ value at each pixel. Use
        \underline{\emph{z-buffer (2D buffer)}} of the same size as image to
        save $z$ values.
        \begin{itemize}
            \item[+] facilitates hardware implementation
            \item[+] handles intersections and cycles
            \item[+] draw opaque polygons in any order
        \end{itemize} 
\end{itemize} 

\subsection{Anti-aliasing}

\begin{itemize}
    \item apply a degree of blurring to the boundary such that the
        aliasing effect is reduced.
    \item \underline{\textbf{Supersampling}}
        \begin{enumerate}
            \item Compute the picture at a high resolution to that of the
                display area.
            \item Supersamples are averaged to find the pixel value.
            \item This blurs the boundaries and leaves the coherent areas of
                color unchanged.
        \end{enumerate} 
    \item \underline{\textbf{Convolution filtering}}: use a filter that takes a
        (weighted) average over a small region around the pixel to blur the
        image, such as
        \[
            \frac{1}{36}
            \begin{pmatrix}
                1 & 4 & 1 \\
                4 & 16 & 4 \\
                1 & 4 & 1
            \end{pmatrix} 
        \]
        \begin{itemize}
            \item[+] very fast, can be done in hardware
            \item[+] generally applicable
            \item[-] degrade the image while enhancing its visual appearance
        \end{itemize} 
\end{itemize} 

\end{document}
